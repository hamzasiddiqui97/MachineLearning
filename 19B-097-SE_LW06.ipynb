{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd \n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from sklearn.datasets import load_iris"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[1]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ames = [\"ID\",\"sepalLength\",\"sepalWidth\",\"petalLength\",\"petalWidth\",\"species\"]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data = load_iris()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split \n", "X = data.data\n", "y = data.target\n", "X_train , X_test , y_train , y_test = train_test_split (X , y , random_state =0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[77]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn . tree import DecisionTreeClassifier\n", "clf = DecisionTreeClassifier ( criterion =\"gini\", max_depth =4 , random_state =1)\n", "clf . fit ( X_train , y_train )"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# TREE STRUCTURE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[19]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["n_nodes = clf . tree_ . node_count\n", "children_left = clf . tree_ . children_left\n", "children_right = clf . tree_ . children_right\n", "feature = clf . tree_ . feature\n", "threshold = clf . tree_ . threshold"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["node_depth = np . zeros ( shape = n_nodes , dtype = np . int64 )\n", "is_leaves = np . zeros ( shape = n_nodes , dtype = bool )\n", "stack = [(0 , 0) ] # start with the root node id (0) and its depth (0)\n", "while len( stack ) > 0:\n", "    # \u00e2\u20ac\u02dcpop \u00e2\u20ac\u02dc ensures each node is only visited once\n", "    node_id , depth = stack . pop ()\n", "    node_depth [ node_id ] = depth\n", "    # If the left and right child of a node is not the same we have a split\n", "    # node\n", "    is_split_node = children_left [ node_id ] != children_right [ node_id ]\n", "    # If a split node , append left and right children and depth to \u00e2\u20ac\u02dcstack \u00e2\u20ac\u02dc\n", "    # so we can loop through them\n", "    if is_split_node :\n", "        stack . append (( children_left [ node_id ] , depth + 1) )\n", "        stack . append (( children_right [ node_id ] , depth + 1) )\n", "    else :\n", "        is_leaves [ node_id ] = True\n", "print (\n", "    \" The binary tree structure has {n} nodes and has \"\n", "    \" the following tree structure :\\n\". format ( n = n_nodes )\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[18]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for i in range ( n_nodes ) :\n", "    if is_leaves [ i ]:\n", "        print(\n", "            \"{space} node ={node} is a leaf node .\". format (\n", "                space = node_depth [ i ] * \"\\t\", node = i))\n", "    else :\n", "        print(\n", "            \"{space} node ={node} is a split node : \"\n", "            \"go to node {left} if X[: , {feature}] <= {threshold} \"\n", "            \" else to node {right}.\". format (\n", "                space = node_depth [i] * \"\\t\",\n", "                node =i,\n", "                left = children_left [i],\n", "                feature = feature [i],\n", "                threshold = threshold [i],\n", "                right = children_right [i]))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[12]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn import tree\n", "plt.figure(figsize=(10,10))\n", "tree . plot_tree ( clf )\n", "plt . show ()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# DECISION PATH"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[13]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["node_indicator = clf . decision_path ( X_test )\n", "leaf_id = clf . apply ( X_test )\n", "sample_id = 0\n", "# obtain ids of the nodes \u00e2\u20ac\u02dcsample_id \u00e2\u20ac\u02dc goes through , i.e. , row \u00e2\u20ac\u02dcsample_id \u00e2\u20ac\u02dc\n", "node_index = node_indicator . indices [\n", "    node_indicator . indptr [ sample_id ] : node_indicator . indptr [ sample_id + 1]\n", "]\n", "print (\" Rules used to predict sample {id}:\\n\". format (id= sample_id ) )\n", "for node_id in node_index :\n", "    # continue to the next node if it is a leaf node\n", "    if leaf_id [ sample_id ] == node_id :\n", "        continue\n", "    # check if value of the split feature for sample 0 is below threshold\n", "    if X_test [ sample_id , feature [ node_id ]] <= threshold [ node_id ]:\n", "        threshold_sign = \"<=\"\n", "    else :\n", "        threshold_sign = \">\"\n", "    print (\n", "        \" decision node {node} : ( X_test [{sample} , {feature}] = {value}) \"\n", "        \"{inequality} {threshold})\". format (\n", "        node = node_id ,\n", "        sample = sample_id ,\n", "        feature = feature [ node_id ] ,\n", "        value = X_test [ sample_id , feature [ node_id ]] ,\n", "        inequality = threshold_sign ,\n", "        threshold = threshold [ node_id ] ,))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# ID3 ALGORITHM:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[52]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class ID3 :\n", "    def __init__ (self , data , t_label ) :\n", "        self . data = data\n", "        self . t_label = t_label\n", "    def getFeatureEntropy ( self , data = None , t_label = None ) :\n", "        if data == None:\n", "            data = self.data\n", "        if t_label == None :\n", "            t_label = self . t_label\n", "        target = data [ t_label ]\n", "        class_list = target.unique()\n", "        total_row = data.shape [0] # the total size of the dataset\n", "        total_entr = 0\n", "        # print (\" Total rows \" , total_row )\n", "        for c in class_list : # for each class in the label\n", "            total_class_count = data [ data [ t_label ] == c ]. shape [0] # number of the class\n", "        # print ( \u00e2\u20ac\u2122 Row with \"{}\" class {} \u00e2\u20ac\u2122. format (c, total_class_count ))\n", "        # print ( \u00e2\u20ac\u2122 Entropy of class \"{0}\" is {1}/{2} * log2 ({1}/{2}) \u00e2\u20ac\u2122.format (c, total_class_count , total_row ))\n", "            total_class_entr = - ( total_class_count / total_row ) * np . log2 (total_class_count / total_row ) # entropy of the class\n", "            total_entr += total_class_entr # adding the class entropy to the total entropy of the dataset\n", "        return total_entr\n", "    \n", "    def get_rem_by_entropy(self) :\n", "        desc_features = pd . DataFrame ()\n", "        target_feature = pd . DataFrame ()\n", "        feature = pd . DataFrame ()\n", "        desc_features = self . data . drop ([ self . t_label ] , axis =1)\n", "        target_feature = self . data [ self . t_label ]\n", "        target_list = list ()\n", "        target_list = target_feature . unique ()\n", "        class_count = desc_features . shape [0]\n", "        rem_list = list ()\n", "        entropy = 0\n", "        class_list = list ()\n", "        feature_list = list ()\n", "        feature_list = desc_features . columns\n", "        for item in feature_list :\n", "            # print ( \u00e2\u20ac\u2122 fetaure : \u00e2\u20ac\u2122 , item )\n", "            rem_feature_entropy =0\n", "            class_list = desc_features [ item ]. unique ()\n", "            new_feature = desc_features [ item ]\n", "            for level in class_list :\n", "                label_class_count = desc_features [desc_features [item] == level].shape [0]\n", "                entropy_class = 0\n", "                feature_level_entropy =0\n", "                sum_feature_entropy =0\n", "                # print ( \u00e2\u20ac\u2122 level \"{0}\" of feature \"{1}\" total count {2} \u00e2\u20ac\u2122. format(level ,item , label_class_count ))\n", "                if label_class_count != 0:\n", "                    probability_class = label_class_count / class_count #probability of the class\n", "                    # print ( \u00e2\u20ac\u2122 Probability value of {0}/{1} is {2:.4 f} \u00e2\u20ac\u2122. format (label_class_count , class_count , probability_class ))\n", "                    for tvalue in target_list :\n", "                        count_level_frequency =0\n", "                        for i in range ( class_count ):\n", "                            if ( new_feature [ i ] == level ) and ( target_feature[ i ] == tvalue ):\n", "                                count_level_frequency +=1\n", "                        if count_level_frequency !=0:\n", "                            feature_prob = count_level_frequency / label_class_count\n", "                            feature_level_entropy = - ( feature_prob * np .log2 ( feature_prob ) )\n", "                            # feature level entropy\n", "                            # print ( \u00e2\u20ac\u2122 pribability {0}/{1} is {2:.4 f} of target\n", "                            #value {3} \u00e2\u20ac\u2122. format ( count_level_frequency , label_class_count ,\n", "                            #feature_level_entropy , tvalue ))\n", "                            sum_feature_entropy += feature_level_entropy\n", "                    ProbXfeature_entropy = probability_class * sum_feature_entropy\n", "                rem_feature_entropy += ProbXfeature_entropy\n", "                # print ( \u00e2\u20ac\u2122 Feature {0} entropy is: {1} \u00e2\u20ac\u2122. format (item ,rem_feature_entropy ))\n", "            rem_list . append (rem_feature_entropy)\n", "        return rem_list\n", "    \n", "    def getInfoGain_by_entropy ( self ) :\n", "        IG_list = list ()\n", "        target_entropy = self . getFeatureEntropy ()\n", "        rem = self . get_rem_by_entropy ()\n", "        for i in range (len( rem ) ) :\n", "            IG_list . append ( target_entropy - rem [ i ])\n", "        return IG_list\n", "    \n", "    def getGR_by_entropy ( self ) :\n", "        data = self . data\n", "        target_label = self . t_label\n", "        feature_list = list ()\n", "        GR_list = list ()\n", "        count =0\n", "        desc_features = data . drop ([ target_label ] , axis =1)\n", "        feature_list = desc_features . columns\n", "        IG = self .getInfoGain_by_entropy()\n", "        for item in feature_list :\n", "            feat_entropy = self . getFeatureEntropy ( None , item )\n", "            # print ( feat_entropy )\n", "            tt = IG [ count ]/ feat_entropy\n", "            GR_list . append ( tt )\n", "            count +=1\n", "        return GR_list"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[73]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dataset = pd.read_csv('vegetation.csv')\n", "id3 = ID3(dataset, 'Vegetation')\n", "print('Entropy: ', id3.getFeatureEntropy(None,None))\n", "print('Remainder Values by entropy method:',id3.get_rem_by_entropy())\n", "print('Information Gain (IG) of each feature:',id3.getInfoGain_by_entropy())\n", "print('Gain Ratio (GR) of rach feature:',id3.getGR_by_entropy())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# LAB TASKS:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# GINI METHOD:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[28]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class GINI:\n", "    def __init__ (self , data , t_label ) :\n", "        self . data = data\n", "        self . t_label = t_label\n", "    def getFeatureEntropy_by_gini ( self,data = None ,t_label = None):\n", "        if data == None:\n", "            data = self.data\n", "        if t_label == None :\n", "            t_label = self . t_label\n", "        target = data [ t_label ]\n", "        class_list = target.unique()\n", "        total_row = data.shape [0] # the total size of the dataset\n", "        total_entr = 0\n", "        # print (\" Total rows \" , total_row )\n", "        for c in class_list : # for each class in the label\n", "            total_class_count = data [ data [ t_label ] == c ]. shape [0] # number of the class\n", "        # print ( \u00e2\u20ac\u2122 Row with \"{}\" class {} \u00e2\u20ac\u2122. format (c, total_class_count ))\n", "        # print ( \u00e2\u20ac\u2122 Entropy of class \"{0}\" is {1}/{2} * log2 ({1}/{2}) \u00e2\u20ac\u2122.format (c, total_class_count , total_row ))\n", "            total_class_entr =  ( total_class_count / total_row )**2 # entropy of the class\n", "            total_entr += total_class_entr # adding the class entropy to the total entropy of the dataset\n", "        return 1-total_entr\n", "        \n", "    def get_rem_by_entropy_gini(self) :\n", "        desc_features = pd . DataFrame ()\n", "        target_feature = pd . DataFrame ()\n", "        feature = pd . DataFrame ()\n", "        desc_features = self . data . drop ([ self . t_label ] , axis =1)\n", "        target_feature = self . data [ self . t_label ]\n", "        target_list = list ()\n", "        target_list = target_feature . unique ()\n", "        class_count = desc_features . shape [0]\n", "        rem_list = list ()\n", "        entropy = 0\n", "        class_list = list ()\n", "        feature_list = list ()\n", "        feature_list = desc_features . columns\n", "        for item in feature_list :\n", "            # print ( \u00e2\u20ac\u2122 fetaure : \u00e2\u20ac\u2122 , item )\n", "            rem_feature_entropy =0\n", "            class_list = desc_features [ item ]. unique ()\n", "            new_feature = desc_features [ item ]\n", "            for level in class_list :\n", "                label_class_count = desc_features [desc_features [item] == level].shape [0]\n", "                entropy_class = 0\n", "                feature_level_entropy =0\n", "                sum_feature_entropy =0\n", "                if label_class_count != 0:\n", "                    probability_class = label_class_count / class_count #probability of the class\n", "                    for tvalue in target_list :\n", "                        count_level_frequency =0\n", "                        for i in range ( class_count ):\n", "                            if ( new_feature [ i ] == level ) and ( target_feature[ i ] == tvalue ):\n", "                                count_level_frequency +=1\n", "                        if count_level_frequency !=0:\n", "                            feature_prob = count_level_frequency / label_class_count\n", "                            feature_level_entropy = ( feature_prob )**2\n", "                            #feature_level_entropy =( total_class_count / total_row )**2\n", "                            sum_feature_entropy += feature_level_entropy\n", "                    ProbXfeature_entropy = probability_class * (1-sum_feature_entropy)\n", "                rem_feature_entropy += ProbXfeature_entropy\n", "            rem_list . append (rem_feature_entropy)\n", "        return rem_list\n", "    \n", "    def getInfoGain_by_gini ( self ) :\n", "        IG_list = list ()\n", "        target_entropy = self .getFeatureEntropy_by_gini ()\n", "        rem = self.get_rem_by_entropy_gini()\n", "        for i in range (len( rem ) ) :\n", "            IG_list . append ( target_entropy - rem [ i ])\n", "        return IG_list\n", "    \n", "    def getGR_by_gini ( self ) :\n", "        data = self . data\n", "        target_label = self . t_label\n", "        feature_list = list ()\n", "        GR_list = list ()\n", "        count =0\n", "        desc_features = data . drop ([ target_label ] , axis =1)\n", "        feature_list = desc_features . columns\n", "        IG = self .getInfoGain_by_gini()\n", "        for item in feature_list :\n", "            feat_entropy = self .getFeatureEntropy_by_gini ( None , item )\n", "            # print ( feat_entropy )\n", "            tt = IG [ count ]/ feat_entropy\n", "            GR_list . append ( tt )\n", "            count +=1\n", "        return GR_list"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dataset = pd.read_csv('vegetation.csv')\n", "Gini = GINI(dataset, 'Vegetation')\n", "print(\"Gini Index:\",Gini.getFeatureEntropy_by_gini())\n", "print(\"------------------------------------------------------------------\")\n", "print('Remainder Values by gini index method:',Gini.get_rem_by_entropy_gini())\n", "print('Information Gain (IG) of each feature by gini:',Gini.getInfoGain_by_gini())\n", "print('Gain Ratio (GR) of rach feature by gini:',Gini.getGR_by_gini())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[84]:"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}