{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# LAB TASKS:<br>\n", "In[21]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "df = pd.read_csv('diabetes.csv')\n", "X = df.drop(['Outcome'],axis=1)\n", "y = df['Outcome']\n", "# # TASK 1-2\n", "# In[22]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn import model_selection\n", "from sklearn.ensemble import BaggingClassifier\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.ensemble import AdaBoostClassifier\n", "from sklearn.metrics import f1_score,accuracy_score\n", "from sklearn.linear_model import LogisticRegression\n", "import numpy as np"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train , X_test , y_train , y_test = train_test_split (X , y , random_state =0)\n", "max_features = 3\n", "kfold = model_selection . KFold ( n_splits =10 , shuffle = True , random_state =2020)\n", "rf = DecisionTreeClassifier ( max_features = max_features )\n", "num_trees = 100\n", "model = BaggingClassifier ( base_estimator = rf , n_estimators = num_trees ,\n", "random_state =2020)\n", "results = model_selection . cross_val_score ( model , X_train ,y_train,cv = kfold )\n", "print (\" Accuracy : %0.2f (+/ - %0.2f)\" % (results . mean () , results . std () ) )"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Random Forest Classification<br>\n", "In[23]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["kfold = model_selection . KFold ( n_splits =10 , shuffle = True , random_state =2020)\n", "rf = DecisionTreeClassifier ()\n", "num_trees = 100\n", "max_features = 3\n", "kfold = model_selection . KFold ( n_splits =10 , shuffle = True , random_state =2020)\n", "model = RandomForestClassifier ( n_estimators = num_trees,max_features =max_features )\n", "results = model_selection.cross_val_score ( model,X_train,y_train,cv =kfold)\n", "print (\" Accuracy : %0.2f (+/ - %0.2f)\" % (results.mean(), results.std()))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[24]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clf_boosting = AdaBoostClassifier (\n", "DecisionTreeClassifier ( max_depth =1) ,\n", "n_estimators =200)\n", "clf_boosting.fit(X_train ,y_train )\n", "predictions =clf_boosting .predict ( X_test )\n", "print(\" For Boosting : F1 Score {} , Accuracy {}\".format(round(f1_score(y_test, predictions),2),round(accuracy_score(y_test,predictions),2)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Random Forest as a Bagging classifier<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[29]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clf_bagging = RandomForestClassifier ( n_estimators =200 , max_depth =1)\n", "clf_bagging . fit ( X_train , y_train )\n", "predictions = clf_bagging . predict ( X_test )\n", "print (\" For Bagging : F1 Score {} , Accuracy {}\". format ( round ( f1_score ( y_test ,\n", "predictions ) ,2) ,round ( accuracy_score ( y_test , predictions ) ,2) ) )"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Comparison Bagging, Boosting and Stacking"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[38]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["boosting_clf_ada_boost = AdaBoostClassifier (\n", "DecisionTreeClassifier ( max_depth =1) ,\n", "n_estimators =3)\n", "bagging_clf_rf = RandomForestClassifier ( n_estimators =200 , max_depth =1,random_state =2020)\n", "clf_rf = RandomForestClassifier ( n_estimators =200 , max_depth =1 , random_state=2020)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[39]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clf_ada_boost = AdaBoostClassifier (\n", "DecisionTreeClassifier ( max_depth =1 , random_state =2020) ,\n", "n_estimators =3)\n", "clf_logistic_reg = LogisticRegression ( solver ='liblinear', random_state =2020)\n", "# Customizing and Exception message"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class NumberOfClassifierException ( Exception ) :\n", "    pass\n", "# Creating a stacking class\n", "class Stacking () :\n", "    def __init__ ( self , classifiers ) :\n", "        if(len( classifiers ) < 2) :\n", "            raise numberOfClassifierException (\" You must fit your classifier with 2 classifiers at least\")\n", "        else :\n", "            self . _classifiers = classifiers\n", "    def fit ( self , data_x , data_y ) :\n", "        stacked_data_x = data_x . copy ()\n", "        for classfier in self . _classifiers [: -1]:\n", "            classfier.fit ( data_x , data_y )\n", "            stacked_data_x = np . column_stack (( stacked_data_x , classfier .\n", "            predict_proba ( data_x ) ) )\n", "        last_classifier = self . _classifiers [ -1]\n", "        last_classifier . fit ( stacked_data_x , data_y )\n", "    def predict ( self , data_x ) :\n", "        stacked_data_x = data_x.copy ()\n", "        for classfier in self._classifiers [: -1]:\n", "            prob_predictions = classfier . predict_proba ( data_x )\n", "            stacked_data_x = np.column_stack (( stacked_data_x ,\n", "            prob_predictions))\n", "        last_classifier = self. _classifiers [ -1]\n", "        return last_classifier. predict(stacked_data_x )\n", "bagging_clf_rf.fit(X_train , y_train)\n", "boosting_clf_ada_boost.fit(X_train,y_train)\n", "classifers_list = [ clf_rf ,clf_ada_boost , clf_logistic_reg ]\n", "clf_stacking = Stacking ( classifers_list )"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clf_stacking.fit(X_train,y_train )\n", "predictions_bagging = bagging_clf_rf . predict ( X_test )\n", "predictions_boosting = boosting_clf_ada_boost . predict ( X_test )\n", "predictions_stacking = clf_stacking . predict ( X_test )\n", "print (\" For Bagging : F1 Score {} , Accuracy {}\". format ( round ( f1_score ( y_test ,\n", "predictions_bagging ) ,2) ,round ( accuracy_score ( y_test , predictions_bagging )\n", ",2) ) )\n", "print (\" For Boosting : F1 Score {} , Accuracy {}\". format ( round ( f1_score ( y_test\n", ", predictions_boosting ) ,2) ,round ( accuracy_score ( y_test ,\n", "predictions_boosting ) ,2) ) )"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[40]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print (\" For Stacking : F1 Score {} , Accuracy {}\". format ( round ( f1_score ( y_test\n", ", predictions_stacking ) ,2) ,round ( accuracy_score ( y_test ,\n", "predictions_stacking ) ,2) ) )"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# TASK 3-4:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[41]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn import model_selection\n", "from sklearn.ensemble import BaggingClassifier\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.datasets import load_breast_cancer\n", "from sklearn.ensemble import AdaBoostClassifier\n", "from sklearn.metrics import f1_score,accuracy_score\n", "from sklearn.linear_model import LogisticRegression\n", "import numpy as np\n", "import pandas as pd\n", "dataset = load_breast_cancer()\n", "X = dataset.data\n", "y = dataset.target\n", "X_train , X_test , y_train , y_test = train_test_split (X , y , random_state =0)\n", "max_features = 3\n", "kfold = model_selection . KFold ( n_splits =10 , shuffle = True , random_state =2020)\n", "rf = DecisionTreeClassifier ( max_features = max_features )\n", "num_trees = 100\n", "model = BaggingClassifier ( base_estimator = rf , n_estimators = num_trees ,\n", "random_state =2020)\n", "results = model_selection . cross_val_score ( model , X_train ,y_train,cv = kfold )\n", "print (\" Accuracy : %0.2f (+/ - %0.2f)\" % (results . mean () , results . std () ) )"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[42]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clf_boosting = AdaBoostClassifier (\n", "DecisionTreeClassifier ( max_depth =1) ,\n", "n_estimators =200)\n", "clf_boosting.fit(X_train ,y_train )\n", "predictions =clf_boosting .predict ( X_test )\n", "print(\" For Boosting : F1 Score {} , Accuracy {}\".format(round(f1_score(y_test, predictions),2),round(accuracy_score(y_test,predictions),2)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[43]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clf_bagging = RandomForestClassifier ( n_estimators =200 , max_depth =1)\n", "clf_bagging . fit ( X_train , y_train )\n", "predictions = clf_bagging . predict ( X_test )\n", "print (\" For Bagging : F1 Score {} , Accuracy {}\". format ( round ( f1_score ( y_test ,\n", "predictions ) ,2) ,round ( accuracy_score ( y_test , predictions ) ,2) ) )"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Comparison Bagging, Boosting and Stacking<br>\n", "In[44]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["boosting_clf_ada_boost = AdaBoostClassifier (\n", "DecisionTreeClassifier ( max_depth =1) ,\n", "n_estimators =3)\n", "bagging_clf_rf = RandomForestClassifier ( n_estimators =200 , max_depth =1,random_state =2020)\n", "clf_rf = RandomForestClassifier ( n_estimators =200 , max_depth =1 , random_state=2020)\n", "clf_ada_boost = AdaBoostClassifier (\n", "DecisionTreeClassifier ( max_depth =1 , random_state =2020) ,\n", "n_estimators =3)\n", "clf_logistic_reg = LogisticRegression ( solver ='liblinear', random_state =2020)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[45]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class NumberOfClassifierException ( Exception ) :\n", "    pass\n", "# Creating a stacking class\n", "class Stacking () :\n", "    def __init__ ( self , classifiers ) :\n", "        if(len( classifiers ) < 2) :\n", "            raise numberOfClassifierException (\" You must fit your classifier with 2 classifiers at least\")\n", "        else :\n", "            self . _classifiers = classifiers\n", "    def fit ( self , data_x , data_y ) :\n", "        stacked_data_x = data_x . copy ()\n", "        for classfier in self . _classifiers [: -1]:\n", "            classfier.fit ( data_x , data_y )\n", "            stacked_data_x = np . column_stack (( stacked_data_x , classfier .\n", "            predict_proba ( data_x ) ) )\n", "        last_classifier = self . _classifiers [ -1]\n", "        last_classifier . fit ( stacked_data_x , data_y )\n", "    def predict ( self , data_x ) :\n", "        stacked_data_x = data_x.copy ()\n", "        for classfier in self._classifiers [: -1]:\n", "            prob_predictions = classfier . predict_proba ( data_x )\n", "            stacked_data_x = np.column_stack (( stacked_data_x ,\n", "            prob_predictions))\n", "        last_classifier = self. _classifiers [ -1]\n", "        return last_classifier. predict(stacked_data_x )\n", "    \n", "bagging_clf_rf.fit(X_train , y_train)\n", "boosting_clf_ada_boost.fit(X_train,y_train)\n", "classifers_list = [ clf_rf ,clf_ada_boost , clf_logistic_reg ]\n", "clf_stacking = Stacking ( classifers_list )\n", "clf_stacking.fit(X_train,y_train )\n", "predictions_bagging = bagging_clf_rf . predict ( X_test )\n", "predictions_boosting = boosting_clf_ada_boost . predict ( X_test )\n", "predictions_stacking = clf_stacking . predict ( X_test )\n", "print (\" For Bagging : F1 Score {} , Accuracy {}\". format ( round ( f1_score ( y_test ,\n", "predictions_bagging ) ,2) ,round ( accuracy_score ( y_test , predictions_bagging )\n", ",2) ) )\n", "print (\" For Boosting : F1 Score {} , Accuracy {}\". format ( round ( f1_score ( y_test\n", ", predictions_boosting ) ,2) ,round ( accuracy_score ( y_test ,\n", "predictions_boosting ) ,2) ) )"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[46]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print (\" For Stacking : F1 Score {} , Accuracy {}\". format ( round ( f1_score ( y_test\n", ", predictions_stacking ) ,2) ,round ( accuracy_score ( y_test ,\n", "predictions_stacking ) ,2) ) )"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# TASK 5-6"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[48]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn import preprocessing\n", "train_data = pd.read_csv(\"train.csv\")\n", "test_data = pd.read_csv(\"test.csv\")\n", "train_data.head(10)\n", "test_ids  = test_data[\"PassengerId\"]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["lean data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def clean(data):\n", "    data = data.drop([\"Ticket\",\"Cabin\",\"PassengerId\",\"Name\"], axis = 1)\n", "    col = [\"SibSp\",\"Parch\",\"Fare\",\"Age\"]\n", "    for i in col:\n", "        data[i].fillna(data[i].median(), inplace = True)\n", "    data.Embarked.fillna(\"U\", inplace = True) #Fill those empty data points with an unknown token\n", "    return data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_data = clean(train_data)\n", "test_data = clean(test_data)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["label_encod = preprocessing.LabelEncoder()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["cols = [\"Sex\",\"Embarked\"]\n", "for col in cols:\n", "    train_data[col] = label_encod.fit_transform(train_data[col])\n", "    test_data[col] = label_encod.transform(test_data[col])\n", "y_pred = train_data[\"Survived\"]\n", "x_pred = train_data.drop(\"Survived\", axis=1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train, X_test, y_train, y_test = train_test_split(x_pred, y_pred, test_size  = 0.2, random_state = 42)\n", "kfold = model_selection . KFold ( n_splits =10 , shuffle = True , random_state =2020)\n", "rf = DecisionTreeClassifier ( max_features = max_features )\n", "num_trees = 100\n", "model = BaggingClassifier ( base_estimator = rf , n_estimators = num_trees ,\n", "random_state =2020)\n", "results = model_selection . cross_val_score ( model , X_train ,y_train,cv = kfold )\n", "print (\" Accuracy : %0.2f (+/ - %0.2f)\" % (results . mean () , results . std () ) )"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[49]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clf_boosting = AdaBoostClassifier (\n", "DecisionTreeClassifier ( max_depth =1) ,\n", "n_estimators =200)\n", "clf_boosting.fit(X_train ,y_train )\n", "predictions =clf_boosting .predict ( X_test )\n", "print(\" For Boosting : F1 Score {} , Accuracy {}\".format(round(f1_score(y_test, predictions),2),round(accuracy_score(y_test,predictions),2)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[50]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clf_bagging = RandomForestClassifier ( n_estimators =200 , max_depth =1)\n", "clf_bagging . fit ( X_train , y_train )\n", "predictions = clf_bagging . predict ( X_test )\n", "print (\" For Bagging : F1 Score {} , Accuracy {}\". format ( round ( f1_score ( y_test ,\n", "predictions ) ,2) ,round ( accuracy_score ( y_test , predictions ) ,2) ) )"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Comparison Bagging, Boosting and Stacking<br>\n", "In[51]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["boosting_clf_ada_boost = AdaBoostClassifier (\n", "DecisionTreeClassifier ( max_depth =1) ,\n", "n_estimators =3)\n", "bagging_clf_rf = RandomForestClassifier ( n_estimators =200 , max_depth =1,random_state =2020)\n", "clf_rf = RandomForestClassifier ( n_estimators =200 , max_depth =1 , random_state=2020)\n", "clf_ada_boost = AdaBoostClassifier (\n", "DecisionTreeClassifier ( max_depth =1 , random_state =2020) ,\n", "n_estimators =3)\n", "clf_logistic_reg = LogisticRegression ( solver ='liblinear', random_state =2020)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[52]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class NumberOfClassifierException ( Exception ) :\n", "    pass\n", "# Creating a stacking class\n", "class Stacking () :\n", "    def __init__ ( self , classifiers ) :\n", "        if(len( classifiers ) < 2) :\n", "            raise numberOfClassifierException (\" You must fit your classifier with 2 classifiers at least\")\n", "        else :\n", "            self . _classifiers = classifiers\n", "    def fit ( self , data_x , data_y ) :\n", "        stacked_data_x = data_x . copy ()\n", "        for classfier in self . _classifiers [: -1]:\n", "            classfier.fit ( data_x , data_y )\n", "            stacked_data_x = np . column_stack (( stacked_data_x , classfier .\n", "            predict_proba ( data_x ) ) )\n", "        last_classifier = self . _classifiers [ -1]\n", "        last_classifier . fit ( stacked_data_x , data_y )\n", "    def predict ( self , data_x ) :\n", "        stacked_data_x = data_x.copy ()\n", "        for classfier in self._classifiers [: -1]:\n", "            prob_predictions = classfier . predict_proba ( data_x )\n", "            stacked_data_x = np.column_stack (( stacked_data_x ,\n", "            prob_predictions))\n", "        last_classifier = self. _classifiers [ -1]\n", "        return last_classifier. predict(stacked_data_x )\n", "    \n", "bagging_clf_rf.fit(X_train , y_train)\n", "boosting_clf_ada_boost.fit(X_train,y_train)\n", "classifers_list = [ clf_rf ,clf_ada_boost , clf_logistic_reg ]\n", "clf_stacking = Stacking ( classifers_list )\n", "clf_stacking.fit(X_train,y_train )\n", "predictions_bagging = bagging_clf_rf . predict ( X_test )\n", "predictions_boosting = boosting_clf_ada_boost . predict ( X_test )\n", "predictions_stacking = clf_stacking . predict ( X_test )\n", "print (\" For Bagging : F1 Score {} , Accuracy {}\". format ( round ( f1_score ( y_test ,\n", "predictions_bagging ) ,2) ,round ( accuracy_score ( y_test , predictions_bagging )\n", ",2) ) )\n", "print (\" For Boosting : F1 Score {} , Accuracy {}\". format ( round ( f1_score ( y_test\n", ", predictions_boosting ) ,2) ,round ( accuracy_score ( y_test ,\n", "predictions_boosting ) ,2) ) )"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[53]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print (\" For Stacking : F1 Score {} , Accuracy {}\". format ( round ( f1_score ( y_test\n", ", predictions_stacking ) ,2) ,round ( accuracy_score ( y_test ,\n", "predictions_stacking ) ,2) ) )"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}